{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIØ4317 Empirical Mini Project\n",
    "## Problem Statement\n",
    "\n",
    "In our empirical mini project, we have chosen to examine the Oslo Børs Benchmark Index (OSEBX) using two distinct forecasting approaches. The reason we have chosen to examine the OSEBX is because of the importance of predicting stock market returns for investors and policymakers.  We have decided on the following research question:\n",
    "\n",
    "***Does univariate time series models like improve the prediction of Oslo Børs Benchmark Index (OSEBX) returns compared to a multiple linear regression model using the macroeconomic indicators consumer price index (CPI), interest rate, and exchange rate (USD/NOK)?***\n",
    "\n",
    "Traditional econometric models, such as multiple linear regression, rely on macroeconomic indicators to explain market movements, while time series models, such as ARIMA, focus on patterns within historical price data. By comparing these two approaches, we seek to determine whether incorporating historical return patterns improves predictive performance relative to a model based on key macroeconomic variables. This research question aligns with the focus on empirical analysis in the course and contributes to a deeper understanding of the strengths and limitations of different forecasting methods in financial markets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from arch import arch_model\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_breusch_godfrey, het_arch, acorr_ljungbox\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tabulate import tabulate\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #Filtering out frequency warnings for readabilty of output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading the different csv-files that contain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "data_path = \"data/\"\n",
    "\n",
    "# Load datasets\n",
    "zero_coupon = pd.read_csv(os.path.join(data_path, \"zero_coupon_rates.csv\"), delimiter=\";\")\n",
    "exchange_rates = pd.read_csv(os.path.join(data_path, \"usd_nok.csv\"), delimiter=\";\")\n",
    "inflation = pd.read_csv(os.path.join(data_path, \"kpi.csv\"), delimiter=\";\")\n",
    "osebx = pd.read_csv(os.path.join(data_path, \"osebx_prices.csv\"), delimiter=\";\")\n",
    "\n",
    "# Replace commas with dots in the KPI column and convert to float\n",
    "inflation[\"kpi\"] = inflation[\"kpi\"].str.replace(\",\", \".\").astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "Then, we check for missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"OSEBX\": osebx,\n",
    "    \"Zero Coupon\": zero_coupon,\n",
    "    \"Exchange Rates\": exchange_rates,\n",
    "    \"Inflation\": inflation\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    missing = df.isnull().sum()\n",
    "    missing_vars = missing[missing > 0]\n",
    "\n",
    "    if missing_vars.empty: \n",
    "        print(f\"{name}: No missing values.\")\n",
    "    else: # If missing values are present, print in which variables it is found. \n",
    "        print(f\"{name}: Missing values in {', '.join(missing_vars.index)}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the data does not contain any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Datetime format\n",
    "Next, we convert the dates to Datetime format.\n",
    "\n",
    "Since the inflation data is originally available only on a monthly basis, we apply linear interpolation to estimate daily values. We have chosen to apply linear interpolation instead of for example forward fill because CPI increases approximately linearly over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to Datetime format\n",
    "osebx[\"Date\"] = pd.to_datetime(osebx[\"Date\"])\n",
    "zero_coupon[\"TIME_PERIOD\"] = pd.to_datetime(zero_coupon[\"TIME_PERIOD\"])\n",
    "exchange_rates[\"TIME_PERIOD\"] = pd.to_datetime(exchange_rates[\"TIME_PERIOD\"])\n",
    "inflation[\"Date\"] = pd.to_datetime(inflation[\"Date\"], format=\"%YM%m\")\n",
    "\n",
    "# Create a full date range from the first to last available date in your dataset\n",
    "full_date_range = pd.date_range(start=inflation[\"Date\"].min(), end=inflation[\"Date\"].max(), freq=\"D\")\n",
    "\n",
    "# Create a DataFrame with daily dates\n",
    "inflation_daily = pd.DataFrame({\"Date\": full_date_range})\n",
    "\n",
    "# Merge with the original inflation data (left join) and forward-fill missing values\n",
    "inflation_daily = inflation_daily.merge(inflation, on=\"Date\", how=\"left\")\n",
    "inflation_daily[\"kpi\"] = inflation_daily[\"kpi\"].interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute log returns for OSEBX\n",
    "It is important for accurate modeling to use log returns instead of index values when analyzing OSEBX. This is because log returns help ensure stationarity, which makes time series models like ARIMA more reliable. \n",
    "\n",
    "Also, rename columns to ensure that all dataframes have a column named \"Date\", so that we can merge all datasets on \"Date\". After having merged all the datasets to one dataframe, we drop all the columns we are not interested in. Consequently, the columns we are left with are \"Date\", \"kpi\", \"zero_coupon_rate\", \"usd_nok_exchange_rate\", and \"log_return\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log returns\n",
    "osebx[\"log_return\"] = np.log(osebx[\"Close\"] / osebx[\"Close\"].shift(1))\n",
    "\n",
    "#For plotting volatility clusters:\n",
    "osebx['sqr_return'] = osebx['log_return']**2\n",
    "osebx['abs_return'] = osebx['log_return'].abs()\n",
    "\n",
    "osebx.dropna(inplace=True)  # Drop the first row where return cannot be calculated\n",
    "\n",
    "# Rename columns to \"Date\"\n",
    "zero_coupon.rename(columns={\"TIME_PERIOD\": \"Date\"}, inplace=True)\n",
    "exchange_rates.rename(columns={\"TIME_PERIOD\": \"Date\"}, inplace=True)\n",
    "\n",
    "# Merge all datasets on 'Date'\n",
    "df = inflation_daily.merge(zero_coupon, on=\"Date\", how=\"inner\")\n",
    "df = df.merge(exchange_rates, on=\"Date\", how=\"inner\")\n",
    "df = df.merge(osebx, on=\"Date\", how=\"inner\") \n",
    "\n",
    "df = df.drop(columns=[\"Close\", \"High\", \"Low\", \"Open\", \"Volume\",\n",
    "       \"FREQ_x\", \"Frequency_x\", \"TENOR_x\", \"Tenor_x\", \"DECIMALS_x\",\n",
    "       \"FREQ_y\", \"Frequency_y\", \"BASE_CUR\", \"Base Currency\",\n",
    "       \"QUOTE_CUR\", \"Quote Currency\", \"TENOR_y\", \"Tenor_y\", \"DECIMALS_y\",\n",
    "       \"CALCULATED\", \"UNIT_MULT\", \"Unit Multiplier\", \"COLLECTION\",\n",
    "       \"Collection Indicator\"])\n",
    "\n",
    "df.rename(columns={\"OBS_VALUE_x\": \"zero_coupon_rate\"}, inplace=True)\n",
    "df.rename(columns={\"OBS_VALUE_y\": \"usd_nok_exchange_rate\"}, inplace=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicollinearity\n",
    "Multicollinearity occurs when two or more independent variables are highly correlated. This makes it difficult to isolate their individual effects on the dependent variable and can lead to unstable coefficient estimates. Therefore, it is important to check for multicollinearity to ensure the model provides reliable estimates and to avoid redundant predictors.\n",
    "\n",
    "\n",
    "We check for multicollinearity by computing the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "selected_columns = [\"kpi\", \"usd_nok_exchange_rate\", \"zero_coupon_rate\"]\n",
    "corr_matrix = df[selected_columns].corr()\n",
    "\n",
    "# Visualize the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define highly correlated values to have a correlation coefficient >0.9. Because all variables have a correlation coefficient below 0.9, we decide to keep all of the explanatory variables. However, we notice that especially between usd_nok_exchange_rate and kpi and between zero_coupon_rate and kpi the correlation coefficient is close to the boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationarity\n",
    "After having checked for correlation, we check for stationarity by applying the Augmented Dickey-Fuller (ADF) test. Stationarity means that a time series has constant statistical properties over time. This includes mean, variance, and autocorrelation. It is crucial to check stationarity because many forecasting models, like ARIMA and regression, assume stationarity for reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "# Check for stationarity \n",
    "def adf_test(series, var_name):\n",
    "    result = adfuller(series.dropna())  \n",
    "    status = \"Stationary\" if result[1] < 0.05 else \"Non-stationary\"\n",
    "\n",
    "    return [var_name, f\"{result[0]:.4f}\", f\"{result[1]:.4f}\", status]\n",
    "\n",
    "# Run ADF tests on all relevant variables\n",
    "stationarity = [\n",
    "    adf_test(df[\"log_return\"], \"Log Return\"),\n",
    "    adf_test(df[\"zero_coupon_rate\"], \"Zero Coupon Rate\"),\n",
    "    adf_test(df[\"usd_nok_exchange_rate\"], \"USD/NOK Exchange Rate\"),\n",
    "    adf_test(df[\"kpi\"], \"CPI\")\n",
    "]\n",
    "\n",
    "# Print results in a formatted table\n",
    "headers = [\"Variable\", \"ADF Statistic\", \"p-value\", \"Result\"]\n",
    "print(tabulate(stationarity, headers=headers, tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that the variables \"zero coupon rate,\" \"USD/NOK exchange rate,\" and \"CPI\" are non-stationary. To ensure stationarity, we apply first differences to these columns. Applying first differences removes trend and stabilizes variance by subtracting the previous value from the current value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to stationary variables by applying first differences\n",
    "df[\"d_kpi\"] = df[\"kpi\"].diff()\n",
    "df[\"d_zero_coupon_rate\"] = df[\"zero_coupon_rate\"].diff()\n",
    "df[\"d_usd_nok_exchange_rate\"] = df[\"usd_nok_exchange_rate\"].diff()\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Run ADF tests after applying first differences on all relevant variables\n",
    "post_diff_stationarity = [\n",
    "    adf_test(df[\"d_zero_coupon_rate\"], \"Zero Coupon Rate\"),\n",
    "    adf_test(df[\"d_usd_nok_exchange_rate\"], \"USD/NOK Exchange Rate\"),\n",
    "    adf_test(df[\"d_kpi\"], \"CPI\")\n",
    "]\n",
    "\n",
    "# Print results in a table format\n",
    "headers = [\"Variable\", \"ADF Statistic\", \"p-value\", \"Stationarity\"]\n",
    "print(tabulate(post_diff_stationarity, headers=headers, tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that all the variables now are stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the explanatory variables used in the Multiple Linear Regression model are also time series of their own, we need to shift all entries, such that they are incorporated as the previous lag. This is to respect the time-dependency of our data. Consequently, we cannot use values unknown at the time of prediction, we can only use the data we actually have at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lag the X-matrix\n",
    "df[\"d_kpi\"] = df[\"d_kpi\"].shift(1)\n",
    "df[\"d_zero_coupon_rate\"] = df[\"d_zero_coupon_rate\"].shift(1)\n",
    "df[\"d_usd_nok_exchange_rate\"] = df[\"d_usd_nok_exchange_rate\"].shift(1)\n",
    "\n",
    "#Drop first rows as these are noe null:\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and testing data\n",
    "We split the data into a training set and a test set in order to evaluate the different models performance on unseen data. The training set is used to fit the model, while the test set assesses how well the model generalizes to new data. By using a separate test set, we ensure that our model’s predictions are not just tailored to historical data, but are robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split percentage (e.g., 80% train, 20% test)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "\n",
    "# Split the data\n",
    "train_df_orig = df.iloc[:split_idx].copy()  # First 80% for training\n",
    "test_df_orig = df.iloc[split_idx:].copy()   # Last 20% for testing\n",
    "\n",
    "# Define training and testing sets\n",
    "X_train_orig, y_train_orig = train_df_orig[['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']], train_df_orig['log_return']\n",
    "X_test_orig, y_test_orig = test_df_orig[['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']], test_df_orig['log_return']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Analysis\n",
    "### Multiple Linear Regression (MLR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_orig.copy()\n",
    "y_train = y_train_orig.copy()\n",
    "\n",
    "X_test = X_test_orig.copy()\n",
    "y_test = y_test_orig.copy()\n",
    "\n",
    "train_df = train_df_orig.copy()\n",
    "test_df = train_df_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocorrelation\n",
    "Start by fitting a model to the training set, and then proceed to check for autocorrelation, which measures the correlation of a variable with itself over successive time intervals. Ignoring autocorrelation leads to inefficient OLS estimates, making standard errors unreliable and increasing the risk of incorrect inferences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant for intercept\n",
    "X_train_ols = sm.add_constant(X_train)\n",
    "\n",
    "# Train OLS model\n",
    "ols_model = sm.OLS(y_train, X_train_ols).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to test for autocorrelation using Breusch-Godfrey test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breusch_godfrey_test(model, nlags=1):\n",
    "    # Perform the Breusch-Godfrey test\n",
    "    lm_stat, p_value, f_stat, f_p_value = acorr_breusch_godfrey(model, nlags=nlags)\n",
    "\n",
    "    # Print the test name and results\n",
    "    print(\"Breusch-Godfrey Test for Autocorrelation\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"LM Statistic: {lm_stat:.4f}\")\n",
    "    print(f\"P-Value: {p_value:.4f}\")\n",
    "    print(f\"F-Statistic: {f_stat:.4f}\")\n",
    "    print(f\"F-Test P-Value: {f_p_value:.4f}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Interpretation of the p-value\n",
    "    if p_value < 0.05:\n",
    "        print(\"Autocorrelation detected in residuals (reject H0).\")\n",
    "    else:\n",
    "        print(\"No significant autocorrelation detected (fail to reject H0).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if autocorrelation is present in our model         \n",
    "breusch_godfrey_test(ols_model, nlags=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the P-value is >0.05, we do not reject the null hypothesis. This means that we do not have evidence to conclude that autocorrelation is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heteroskedasticity\n",
    "We continue by testing for heteroskedasticity. This is important because we assume homoskedasticity, which means that all the errors have the same variance. Identifying heteroskedasticity is important because it affects standard errors, confidence intervals, and hypothesis testing reliability in OLS regression.\n",
    "\n",
    "We test if the homoskedasticity assumption holds by using the ARCH test, as it is well-suited for time-series data because it detects time-dependent variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = ols_model.resid\n",
    "\n",
    "arch_test = het_arch(residuals)\n",
    "\n",
    "print(f\"ARCH Test Statistic: {arch_test[0]}\")\n",
    "print(f\"p-value: {arch_test[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the strong heteroskedasticity identified by the ARCH test, we use heteroskedasticity-consistent (HC) standard error estimates. This is to ensure valid inference in our regression model. Since our training dataset consists of approximately 2000 observations, we choose HC3 since it is well-suited for medium to large sample sizes. This improves the robustness of our statistical inference, meaning we can trust our p-values and confidence intervals despite our model having heteroscedasticity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_robust = ols_model.get_robustcov_results(cov_type='HC3')\n",
    "\n",
    "print(model_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Evaluation\n",
    "Now, we want to evaluate the performance of our Multiple Linear Regression (MLR) model. \n",
    "\n",
    "After having done this, we use the robust model to make predictions and compute the performance metrics Mean Absolute Percentage Error (MAPE), Mean Squared Error (MSE), and Mean Absolute Error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "y_pred = model_robust.predict(X_test)\n",
    "\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  \n",
    "mse = mean_squared_error(y_test, y_pred)  \n",
    "mae = mean_absolute_error(y_test, y_pred) \n",
    "\n",
    "results_MLR = [\n",
    "    [\"Metric\", \"Value\"],\n",
    "    [\"MAPE (%)\", f\"{mape:.4f}\"],\n",
    "    [\"MSE\", f\"{mse:.4f}\"],\n",
    "    [\"MAE\", f\"{mae:.4f}\"]\n",
    "]\n",
    "\n",
    "print(tabulate(results_MLR, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Percentage Error (MAPE)**\n",
    "\n",
    "A MAPE value of **132.275%** indicates that, on average, the model's predictions deviate from the actual values by more than twice their magnitude. This indicates clearly that the model struggles to make accurate predictions, and is likely due to a combination of the model not being able to capture the complex dynamics in the explanatory variables and the fact that log returns are very small values. Therefore, a small mistake in the prediction can lead to a large percentage error.\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "The MSE value of **0.0001** seems quite low, however, this is quite misleading. As noted earlier, the scale of the dependent variable is small, which makes the squared errors appear low even if the relative percentage error is large.\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "The MAE value of **0.0059** represents the average absolute error in raw terms and indicates small deviations in absolute terms. However, the reason for such a low MAE value is the small scale of the dependent variable. This makes the absolute errors to seem small even if the relative error is large. \n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "While the MSE and MAE values appear low, the extremely high MAPE highly suggests that the model performs poorly in predicting the dependent variable. The small scale of the log returns of the OSEBX makes the absolute errors seem small but leads to large percentage errors. Also, the low **adjusted R squared value of 0.005** further confirms that the model has weak explanatory power. \n",
    "\n",
    "Overall, the model is not reliable for forecasting OSEBX returns and requires improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Time Series Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start our time series modeling, we need to perform some exploratory data analysis, that might help tailor how we model the predicting model for our returns. We wish to model the $\\textit{Expected return}$ given the available information at the current timestep, the same way the linear regression model works. The key is to distinguish features from the returns time series itself that help explain the variance in the returns. By testing our MLR model against such a model, we can conclude whether the external macroeconomic factors help explain the variance better or worse than the returns themselves. \n",
    "\n",
    "Extract mean return and empirical standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_return = df['log_return'].mean()\n",
    "std = df['log_return'].std()\n",
    "\n",
    "print(\"Mean return:\", mean_return, \"Standard error:\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the mean return is centered around zero, whilst the volatility is of a significantly larger magnitude. As we have already tested the stationarity of the returns, $and$ shown that the variance is not homoskedastic, we wish to plot the different autocorrelation relationships of our series. But first, lets take a look at the distribution of our return series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the distributed log-returns:\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df['log_return'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Histogram of Log-Returns\")\n",
    "plt.xlabel(\"Log-Return\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the plot shows, we have returns distributed closely around zero, as expected. Furthermore, we see that the tails of our distribution are very widespread, some with return 8 times the standard deviation. Thereby, we find it most reasonable to use a t-distribution when estimating models later on, as opposed to the regular normal distribution assumption, as it more realistically reflects the returns we have observed in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocorrelation \n",
    "Next lets plot ACF and PACF of the returns to check for autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the acf and pacf of the differenced data:\n",
    "time_series = df[['Date', 'log_return', 'sqr_return', 'abs_return']].copy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "plot_acf(time_series['log_return'], lags=40, ax=axes[0])\n",
    "axes[0].set_title('Autocorrelation Function (ACF)')\n",
    "\n",
    "plot_pacf(time_series['log_return'], lags=40, ax=axes[1], method='ywm')\n",
    "axes[1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the ACF and PACF of the logarithmic returns, nothing stands out. In traditional financial time series modeling, logarithmic returns are typically stationary, and behave close to white noise. This fits with the underlying $\\textit{efficient market hypothesis}$, that reflects that returns of assets, such as Oslo Børs, are white noise and thereby $\\textit{not}$ predictable. \n",
    "\n",
    "We still see an interesting spike at 7 days prior, indicating that there might be a seasonal relationship between the returns a week ago and the current return. Therefore we want to test a SARIMA approach to incorporate this insight. The standard ARMA(1,1) is also commonly used as a benchmark model to test predictability of separate models against, so we test this model as well. Since our returns are stationary with mean zero and $\\textit{low to none}$ pure autocorrelation in the asset returns, a pure mean value predictor, commonly referred to as just ARMA(0,0) will also be used as a baseline. The mean value predictor at 0 is essentially the efficient market hypothesis in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = time_series.copy()\n",
    "\n",
    "model_df['Date'] = pd.to_datetime(model_df['Date'])\n",
    "model_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Set frequency explicitly (e.g., business days or daily)\n",
    "model_df = model_df.asfreq('D')  # D' for calendar day\n",
    "\n",
    "# Define the models: (p,d,q) and seasonal (P,D,Q,s)\n",
    "model_configs = {\n",
    "    \n",
    "    #pure ARMA models:\n",
    "    \"ARMA(0,0)\": ((0, 0, 0), (0, 0, 0, 0)),\n",
    "    \"ARMA(1,0)\": ((1, 0, 0), (0, 0, 0, 0)),\n",
    "    \"ARMA(0,1)\": ((0, 0, 1), (0, 0, 0, 0)),\n",
    "    \"ARMA(1,1)\": ((1, 0, 1), (0, 0, 0, 0)),\n",
    "    \n",
    "    #Seasonal models:\n",
    "    \"SARIMA(1,0,1)(1,0,0)[7]\": ((1, 0, 1), (1, 0, 0, 7)),\n",
    "    \"SARIMA(1,0,1)(0,0,1)[7]\": ((1, 0, 1), (0, 0, 1, 7)),\n",
    "    \"SARIMA(1,0,1)(1,0,1)[7]\": ((1, 0, 1), (1, 0, 1, 7)),\n",
    "    \"SARIMA(1,0,0)(1,0,0)[7]\": ((1, 0, 0), (1, 0, 0, 7)),\n",
    "    \"SARIMA(1,0,0)(0,0,1)[7]\": ((1, 0, 0), (0, 0, 1, 7)),\n",
    "    \"SARIMA(1,0,0)(1,0,1)[7]\": ((1, 0, 0), (1, 0, 1, 7)),\n",
    "    \"SARIMA(0,0,1)(1,0,1)[7]\": ((0, 0, 1), (1, 0, 1, 7)),\n",
    "    \"SARIMA(0,0,1)(1,0,0)[7]\": ((0, 0, 1), (1, 0, 0, 7)),\n",
    "    \"SARIMA(0,0,1)(0,0,1)[7]\": ((0, 0, 1), (0, 0, 1, 7)),\n",
    "    \"SARIMA(0,0,0)(1,0,1)[7]\": ((0, 0, 0), (1, 0, 1, 7)),\n",
    "    \"SARIMA(0,0,0)(1,0,0)[7]\": ((0, 0, 0), (1, 0, 0, 7)),\n",
    "    \"SARIMA(0,0,0)(0,0,1)[7]\": ((0, 0, 0), (0, 0, 1, 7)),\n",
    "}\n",
    "\n",
    "# Fit and store results\n",
    "results_summary = {}\n",
    "\n",
    "for name, (order, seasonal_order) in model_configs.items():\n",
    "    model = SARIMAX(\n",
    "        model_df[\"log_return\"],\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order,\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    )\n",
    "    fit = model.fit(disp=False)\n",
    "    results_summary[name] = {\n",
    "        \"AIC\": fit.aic,\n",
    "        \"BIC\": fit.bic,\n",
    "        \"Log-Likelihood\": fit.llf\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for comparison\n",
    "benchmark_results = pd.DataFrame(results_summary).T\n",
    "print(benchmark_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As AIC and BIC provide feedback on the model fit to complexity ratio, we extract he following models as our baseline for the SARIMA, with a tradeoff between complexity, information criterion and log-likelihood:\n",
    "\n",
    "- ARMA(0,0)\n",
    "- ARMA(1,0)\n",
    "- SARIMA(0,0,1)(1,0,0)[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Volatility Clustering \n",
    "Next we plot squared returns and absolute returns to check for volatility clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared Returns Line Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(time_series['sqr_return'], label='Squared Returns')\n",
    "plt.title('Squared Returns of Oslo Børs Index')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Squared Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ACF and PACF of Squared Returns \n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "plot_acf(time_series['sqr_return'].dropna(), lags=40, ax=axes[0])\n",
    "axes[0].set_title(\"ACF of Squared Returns\")\n",
    "plot_pacf(time_series['sqr_return'].dropna(), lags=40, ax=axes[1], method='ywm')\n",
    "axes[1].set_title(\"PACF of Squared Returns\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Absolute Log Returns Line Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(time_series['abs_return'], label='Absolute Log Returns', color='orange')\n",
    "plt.title('Volatility Clustering (Absolute Log Returns)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Absolute Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ACF and PACF of Absolute Returns \n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "plot_acf(time_series['abs_return'].dropna(), lags=40, ax=axes[0])\n",
    "axes[0].set_title(\"ACF of Absolute Log Returns\")\n",
    "plot_pacf(time_series['abs_return'].dropna(), lags=40, ax=axes[1], method='ywm')\n",
    "axes[1].set_title(\"PACF of Absolute Log Returns\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the plots of ACF and PACF of the squared and absolute returns show, there is strong serial dependency in our time series, even though the simple autocorrelation of log-returns might only have a significant spike at lag 7. These results, in addition to the statistically significant ARCH effect of our resiudals from the MLR, serve as a strong intuition for our choice of using volatility modeling for the Oslo Børs returns. To provide further insight we perform the Ljung-Box test on the squared returns series to test for serial autocorrelation. We test the following:\n",
    "\n",
    "$H_0:$ The data is independently distributed, against $H_1:$, we have serial dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Ljung-Box test at multiple lags:\n",
    "ljung_box_result_sqr = acorr_ljungbox(time_series['sqr_return'], lags=[10, 20, 30], return_df=True)\n",
    "\n",
    "print(\"Ljung-Box Test on Squared Returns:\")\n",
    "print(ljung_box_result_sqr)\n",
    "\n",
    "print()\n",
    "\n",
    "ljung_box_result_log = acorr_ljungbox(time_series['log_return'], lags=[10, 20, 30], return_df=True)\n",
    "\n",
    "print(\"Ljung-Box Test on log Returns:\")\n",
    "print(ljung_box_result_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the test shows, we have extremely strong evidence to reject the null hypothesis, and thereby conclude that the data has serial dependencies, and a volatility model is appropriate. we also see that the log-returns have presence of autocorrelation, meaning that we should adjust the residuals for ARCH effects after fitting our ARMA models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Volatility Behavior of the Oslo Børs Index\n",
    "\n",
    "The plots above illustrate the behavior of log returns, their absolute values, and their squared values over time. We clearly observe a **massive spike in volatility** in the middle of the dataset, corresponding to the **Covid-19 outbreak in 2020** — a globally disruptive event that led to extreme uncertainty and financial market instability.\n",
    "\n",
    "Beyond this one-off shock, we also observe **persistent clusters of higher volatility** across different periods, particularly visible in the plots of **absolute** and **squared log returns**. These clusters indicate that **volatility is not constant over time**, but rather shows signs of **volatility clustering** — a well-documented phenomenon in financial markets where large changes tend to be followed by large changes (of either sign), and small changes by small changes.\n",
    "\n",
    "This volatility clustering behavior violates the **homoskedasticity assumption** of ARIMA/SARIMA models, and provides strong empirical justification for applying **GARCH-type models**, which are specifically designed to handle **time-varying conditional variance**. In other words, GARCH models not only acknowledge that volatility changes over time, but also model it explicitly based on past shocks and past volatility. This is appropriate in our case, as the Ljung-Box test shows that the squared returns series has strong serial dependencies.\n",
    "\n",
    "Thus, the Oslo Børs index return data supports that **we should adjust residuals from SARIMA with a GARCH approach** when forecasting financial returns or understanding risk dynamics. We fit the GARCH model instead of the ARCH model due to its ability to capture the persistence of shocks whithout needing a high order of lags.\n",
    "\n",
    "Following the procedure described in the book: Analyzis of Financial Time Series by Tsay et al, the next step after observing the volatility clustering effect -and testing prior residuals for ARCH effects, is to fit a volatility model to the returns. As we have shown previously, ARMA(1,0) has a high log-likelihood compared to the rest of the ARMA models, and will be used as the mean-equation. The idea is that since we have strong serial correlation in the squared returns, and thereby volatility clustering, todays volatility might impact tomorrows return. To capture this hypothetical relationship we explore the GARCH-M model with AR(1). As financial theory assumes risk aversion in the market, we expect investors to react differently to periods of $bad$ volatility compared to $good$ volatility. I.E the risk premium is larger during periods of bad volatility. To captures this skewed effect we use the $\\textbf{Assymetric GARCH-M Model}$. As explained previously, we use the t-distribution as the estimated population distribution of our returns, due to the fat tails we observed in the exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GARCH-M Model\n",
    "We test the GARCH-M model to capture hypothetical volatility drift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Rescale returns for numerical stability (multiply by 100)\n",
    "ts_garch_test = time_series.copy()\n",
    "ts_garch_test['log_return_scaled'] = ts_garch_test['log_return'] * 100\n",
    "\n",
    "# Step 2: Fit an AR(1)-GARCH(1,1) model to the scaled returns\n",
    "# This will give us conditional volatility estimates.\n",
    "garch_model = arch_model(\n",
    "    ts_garch_test['log_return_scaled'],\n",
    "    mean='ARX',       # AR model in the mean\n",
    "    lags=1,           # AR(1)\n",
    "    o = 1,            #Enable assymetric effect of volatility\n",
    "    vol='GARCH',      # GARCH specification for volatility\n",
    "    p=1,              # GARCH order 1\n",
    "    q=1,              # ARCH order 1\n",
    "    dist='t'     # Use the t distribution as return series has fat tails\n",
    ")\n",
    "garch_fit = garch_model.fit(disp='off')\n",
    "\n",
    "# Step 3: Extract the conditional variance and shift by 1 period\n",
    "# We shift because we want yesterday's conditional variance to predict today’s return.\n",
    "ts_garch_test['cond_var'] = garch_fit.conditional_volatility ** 2\n",
    "ts_garch_test['lag_cond_var'] = ts_garch_test['cond_var'].shift(1)\n",
    "\n",
    "# Step 4: Create a lagged return column (AR(1) component)\n",
    "ts_garch_test['lag_return'] = ts_garch_test['log_return_scaled'].shift(1)\n",
    "\n",
    "# Create a copy of the DataFrame slice to avoid warnings\n",
    "df_model = ts_garch_test.dropna(subset=['lag_return', 'lag_cond_var', 'log_return_scaled']).copy()\n",
    "\n",
    "# Assign new columns for bad and good volatility components:\n",
    "df_model.loc[:, 'lag_bad_vol'] = np.where(df_model['lag_return'] < 0, df_model['lag_cond_var'], 0)\n",
    "df_model.loc[:, 'lag_good_vol'] = np.where(df_model['lag_return'] >= 0, df_model['lag_cond_var'], 0)\n",
    "\n",
    "# Step 5: Run an OLS regression to estimate the assymetric GARCH-M model\n",
    "X = sm.add_constant(df_model[['lag_return', 'lag_bad_vol', 'lag_good_vol']])\n",
    "y = df_model['log_return_scaled']\n",
    "ols_model = sm.OLS(y, X).fit()\n",
    "\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results above, we have a low R-squared which is expected for a financial return series. However, we see that the coefficients for good and bad volatility are significant, entailing that the nature of the volatility $\\textit{most likely}$ has a significant impact on the next return, supporting the assumption of investor risk aversion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from the estimated models\n",
    "\n",
    "To use the univariate time-series models to predict the day ahead Oslo Børs returns, we must set up our training and testing data a little different than when we benchmarked the MLR model. We still wish to test its performance on the 20% of unseen returns, but since we use lagged values of the time series itself, we must include the available information at the prediction time, as we only predict $one$ timestep ahead. This is often referred to as $\\textit{rolling one-step-ahead forecasting}$. This means that we re-estimate each model for every new observation, as observations many steps into the future is assumed to be $non-predictable$ and thereby not interesting to evaluate either. By re-calculating the model with the information gained at each new timestep, we make sure that the results we obtain are from actual day-ahead forecasting.\n",
    "\n",
    "As a quick recap here is our univariate time series models we benchmark MLR against:\n",
    "- ARMA(0,0)\n",
    "- ARMA(1,0)\n",
    "- SARIMA(0,0,1),(1,0,0)[7]\n",
    "- GARCH-M (1,1), AR(1)\n",
    "\n",
    "Set up training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Data\n",
    "ts = time_series.copy()\n",
    "ts['log_return_scaled'] = ts['log_return'] * 100\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "n = len(ts)\n",
    "train_size = int(n * 0.8)\n",
    "train_ts = ts.iloc[:train_size].copy()\n",
    "test_ts = ts.iloc[train_size:].copy()\n",
    "\n",
    "# 2. Initialize Storage\n",
    "forecast_dates = []\n",
    "forecast_results = {\n",
    "    'GARCH-M': [],\n",
    "    'ARMA(0,0)': [],\n",
    "    'ARMA(1,0)': [],\n",
    "    'SARIMA(0,0,1)(1,0,0)[7]': [],\n",
    "    'Actual': []\n",
    "}\n",
    "\n",
    "# Rolling training data\n",
    "rolling_ts = train_ts.copy()\n",
    "\n",
    "# 3. Rolling Forecast Loop\n",
    "for t, row in test_ts.iterrows():\n",
    "    forecast_dates.append(t)\n",
    "    y_train = rolling_ts['log_return_scaled'].dropna()\n",
    "\n",
    "    # ARMA(0,0)\n",
    "    try:\n",
    "        model_00 = SARIMAX(y_train, order=(0, 0, 0), enforce_stationarity=False, enforce_invertibility=False)\n",
    "        result_00 = model_00.fit(disp=False)\n",
    "        pred_00 = result_00.forecast(steps=1).iloc[0]\n",
    "    except:\n",
    "        pred_00 = np.nan\n",
    "\n",
    "    # ARMA(1,0)\n",
    "    try:\n",
    "        model_10 = SARIMAX(y_train, order=(1, 0, 0), enforce_stationarity=False, enforce_invertibility=False)\n",
    "        result_10 = model_10.fit(disp=False)\n",
    "        pred_10 = result_10.forecast(steps=1).iloc[0]\n",
    "    except:\n",
    "        pred_10 = np.nan\n",
    "\n",
    "    # SARIMA(0,0,1)(1,0,0)[7]\n",
    "    try:\n",
    "        model_sarima = SARIMAX(y_train,\n",
    "                               order=(0, 0, 1),\n",
    "                               seasonal_order=(1, 0, 0, 7),\n",
    "                               enforce_stationarity=False,\n",
    "                               enforce_invertibility=False)\n",
    "        result_sarima = model_sarima.fit(disp=False)\n",
    "        pred_sarima = result_sarima.forecast(steps=1).iloc[0]\n",
    "    except:\n",
    "        pred_sarima = np.nan\n",
    "\n",
    "    # GARCH-M Model (Asymmetric)\n",
    "    try:\n",
    "        garch_model = arch_model(\n",
    "            y_train,\n",
    "            mean='ARX', lags=1,\n",
    "            o=1, vol='GARCH', p=1, q=1,\n",
    "            dist='t'\n",
    "        )\n",
    "        garch_fit = garch_model.fit(disp='off')\n",
    "\n",
    "        rolling_ts['cond_var'] = garch_fit.conditional_volatility ** 2\n",
    "        rolling_ts['lag_cond_var'] = rolling_ts['cond_var'].shift(1)\n",
    "        rolling_ts['lag_return'] = rolling_ts['log_return_scaled'].shift(1)\n",
    "\n",
    "        df_model = rolling_ts.dropna(subset=['lag_return', 'lag_cond_var', 'log_return_scaled']).copy()\n",
    "        df_model['lag_bad_vol'] = np.where(df_model['lag_return'] < 0, df_model['lag_cond_var'], 0)\n",
    "        df_model['lag_good_vol'] = np.where(df_model['lag_return'] >= 0, df_model['lag_cond_var'], 0)\n",
    "\n",
    "        X = sm.add_constant(df_model[['lag_return', 'lag_bad_vol', 'lag_good_vol']])\n",
    "        y = df_model['log_return_scaled']\n",
    "        ols_model = sm.OLS(y, X).fit()\n",
    "\n",
    "        last_return = rolling_ts['log_return_scaled'].iloc[-1]\n",
    "        last_cond_var = rolling_ts['cond_var'].iloc[-1]\n",
    "\n",
    "        if pd.isna(last_return) or pd.isna(last_cond_var):\n",
    "            pred_garch = np.nan\n",
    "        else:\n",
    "            input_data = {\n",
    "                'const': 1.0,\n",
    "                'lag_return': last_return,\n",
    "                'lag_bad_vol': last_cond_var if last_return < 0 else 0.0,\n",
    "                'lag_good_vol': last_cond_var if last_return >= 0 else 0.0\n",
    "            }\n",
    "            X_fc = pd.DataFrame([input_data])\n",
    "            pred_garch = ols_model.predict(X_fc).iloc[0]\n",
    "    except Exception as e:\n",
    "        print(f\"[GARCH-M] Exception at {t}: {e}\")\n",
    "        pred_garch = np.nan\n",
    "\n",
    "    # Store Results\n",
    "    forecast_results['GARCH-M'].append(pred_garch)\n",
    "    forecast_results['ARMA(0,0)'].append(pred_00)\n",
    "    forecast_results['ARMA(1,0)'].append(pred_10)\n",
    "    forecast_results['SARIMA(0,0,1)(1,0,0)[7]'].append(pred_sarima)\n",
    "    forecast_results['Actual'].append(ts.loc[t, 'log_return_scaled'])\n",
    "\n",
    "    # Update training data with actual value at t\n",
    "    rolling_ts = pd.concat([rolling_ts, ts.loc[[t]]])\n",
    "\n",
    "# 4. Build Final Forecast DataFrame\n",
    "forecast_df = pd.DataFrame(forecast_results, index=forecast_dates)\n",
    "print(forecast_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance evaluation\n",
    "Now we calculate our performance metrics before comparing the results against the MLR model and their implications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert forecasts and actuals back to the original scale by dividing by 100.\n",
    "actual_rescaled = forecast_df['Actual'].values / 100.0\n",
    "models = [\"ARMA(0,0)\", \"ARMA(1,0)\", \"SARIMA(0,0,1)(1,0,0)[7]\", \"GARCH-M\"]\n",
    "\n",
    "results = []\n",
    "for model in models:\n",
    "    preds_rescaled = forecast_df[model].values / 100.0\n",
    "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs((actual_rescaled - preds_rescaled) / actual_rescaled)) * 100\n",
    "    mse = mean_squared_error(actual_rescaled, preds_rescaled)\n",
    "    mae = mean_absolute_error(actual_rescaled, preds_rescaled)\n",
    "    results.append([model, f\"{mape:.5f}\", f\"{mse:.5f}\", f\"{mae:.5f}\"])\n",
    "\n",
    "headers = [\"Model\", \"MAPE (%)\", \"MSE\", \"MAE\"]\n",
    "print(tabulate(results, headers=headers, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the ARMA(0,0) has a MAPE of 100%, as it only predicts 0 for every return, and thereby the error will simply be 100% every time. Even though the difference is very minor, we see that the assymetric GARCH-M model outperforms the other models on MAE and MAPE. As the scale of log-returns is very small, this slight performance increase is still worth considering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We wanted to investigate whether the use of univariate time series models improve the prediction of Oslo Børs Benchmark Index (OSEBX) returns compared to an MLR model using macroeconomic indicators. Based on our result, the GARCH-M model demonstrated the best predictive performance. It outperforms MLR in terms of MAPE and MSE, even though the difference in MAE was minimal.\n",
    "\n",
    "The GARCH-M model achieved the lowest MAPE (105.78%) among the models tested, indicating it provided the most accurate relative predictions. MLR had a significantly higher MAPE (132.27%), which shows greater percentage error in its forecasts. Additionally, all univariate models had a lower MSE (6e-05) compared to MLR (0.0001), meaning they produced smaller squared errors on average. The MAE values were very similar across all models. These findings indicate that the absolute errors remained close between models, while GARCH-M provided better error reduction in terms of percentage deviation and squared errors. \n",
    "\n",
    "However, overall, the predictive performance of all models was poor. A MAPE exceeding 100% suggests that the different models’ predictions deviate from actual returns by more than the true values themselves, on average. This makes them practically useless for financial forecasting. The consistently high errors indicate that OSEBX returns are highly complex and likely influenced by factors not captured in our models. Consequently, this suggests that the underlying dynamics of stock market returns are not effectively captured by neither univariate time series models nor a basic MLR approach.\n",
    "\n",
    "The differences in performance can be connected to the strengths and weaknesses of each modeling approach. The univariate time series models, particularly GARCH-M, are well-suited for capturing time-dependent volatility in financial data. This could suggest that they are effective at modeling stock returns that exhibit periods of high and low variance. However, they do not incorporate external macroeconomic factors, which could be crucial in understanding broader market trends. The poor performance of the ARMA and SARIMA models suggests that simple autoregressive and moving average components alone are insufficient for modeling OSEBX returns. The reason is that the stock market is non-stationary of nature and due to the presence of structural breaks. \n",
    "\n",
    "On the other hand, MLR integrates macroeconomic indicators, which theoretically should enhance predictive accuracy. However, the model suffers from heteroscedasticity, meaning that its error variance changes over time, reducing the reliability of its estimates. Additionally, the high correlation between macroeconomic variables introduces multicollinearity, making it difficult to separate the individual effects of CPI, interest rate, and exchange rate on OSEBX returns. Furthermore, our study relies on data from only the last 10 years. This could result in models that cannot fully capture long-term economic cycles or structural shifts in the market. Moreover, this limits the effectiveness of both modeling approaches.\n",
    "\n",
    "In conclusion, our results indicate that GARCH-M provides better predictive performance than MLR in terms of MAPE and MSE, although the absolute error differences are small. However, because of the overall poor predictive accuracy of all models, neither approach is suitable for reliable forecasting of OSEBX returns. Future research could explore hybrid models that integrate both time series dynamics and macroeconomic indicators or investigate machine learning techniques that may overcome these limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of LLMs\n",
    "### How are LLMs used?\n",
    "We have used ChatGPT for two different purposes listed below:\n",
    "\n",
    "* **Debugging of code:** Sometimes we encountered errors or coding issues difficult to resolve on our own. ChatGPT was then used to get quick explanations of error messages and suggest alternative approaches.  \n",
    "* **Spell check:** When working with writing the report, ChatGPT was used in some parts to check grammar, spelling and phrasing. This contributed to making our writing clearer and more professional.\n",
    "\n",
    "### What are our experiences?\n",
    "Overall, our experience with AI has been positive. It has saved us time, provided quick support when needed and improved our language. However, we recognize that AI is not always correct, making it essential to verify its responses before applying them to our project. This was particularly seen when using ChatGPT for debugging, as it sometimes struggled to understand the specific context of our implementation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
