{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIØ4317 Empirical Mini Project\n",
    "## Introduction\n",
    "Hva er dette prosjektet, formål osv. forklare litt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_breusch_godfrey, het_arch\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading the different csv-files that contain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "data_path = \"data/\"\n",
    "\n",
    "# Load datasets\n",
    "zero_coupon = pd.read_csv(os.path.join(data_path, \"zero_coupon_rates.csv\"), delimiter=\";\")\n",
    "exchange_rates = pd.read_csv(os.path.join(data_path, \"usd_nok.csv\"), delimiter=\";\")\n",
    "inflation = pd.read_csv(os.path.join(data_path, \"kpi.csv\"), delimiter=\";\")\n",
    "osebx = pd.read_csv(os.path.join(data_path, \"osebx_prices.csv\"), delimiter=\";\")\n",
    "\n",
    "# Replace commas with dots in the KPI column and convert to float\n",
    "inflation[\"kpi\"] = inflation[\"kpi\"].str.replace(\",\", \".\").astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Values\n",
    "Then, we check for missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"OSEBX\": osebx,\n",
    "    \"Zero Coupon\": zero_coupon,\n",
    "    \"Exchange Rates\": exchange_rates,\n",
    "    \"Inflation\": inflation\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    missing = df.isnull().sum()\n",
    "    missing_vars = missing[missing > 0]\n",
    "\n",
    "    if missing_vars.empty: \n",
    "        print(f\"{name}: No missing values.\")\n",
    "    else: # If missing values are present, print in which variables it is found. \n",
    "        print(f\"{name}: Missing values in {', '.join(missing_vars.index)}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the data does not contain any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to Datetime format\n",
    "Next, we convert the dates to Datetime format.\n",
    "\n",
    "Since the inflation data is originally available only on a monthly basis, we apply linear interpolation to estimate daily values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to Datetime format\n",
    "osebx[\"Date\"] = pd.to_datetime(osebx[\"Date\"])\n",
    "zero_coupon[\"TIME_PERIOD\"] = pd.to_datetime(zero_coupon[\"TIME_PERIOD\"])\n",
    "exchange_rates[\"TIME_PERIOD\"] = pd.to_datetime(exchange_rates[\"TIME_PERIOD\"])\n",
    "inflation[\"Date\"] = pd.to_datetime(inflation[\"Date\"], format=\"%YM%m\")\n",
    "\n",
    "# Create a full date range from the first to last available date in your dataset\n",
    "full_date_range = pd.date_range(start=inflation[\"Date\"].min(), end=inflation[\"Date\"].max(), freq=\"D\")\n",
    "\n",
    "# Create a DataFrame with daily dates\n",
    "inflation_daily = pd.DataFrame({\"Date\": full_date_range})\n",
    "\n",
    "# Merge with the original inflation data (left join) and forward-fill missing values\n",
    "inflation_daily = inflation_daily.merge(inflation, on=\"Date\", how=\"left\")\n",
    "inflation_daily[\"kpi\"] = inflation_daily[\"kpi\"].interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute log returns for OSEBX\n",
    "Also, rename columns to ensure that all dataframes have a column named \"Date\", so that we can merge all datasets on \"Date\". After having merged all the datasets to one dataframe, we drop all the columns we are not interested in. Consequently, the columns we are left with are \"Date\", \"kpi\", \"zero_coupon_rate\", \"usd_nok_exchange_rate\", and \"log_return\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log returns\n",
    "osebx[\"log_return\"] = np.log(osebx[\"Close\"] / osebx[\"Close\"].shift(1))\n",
    "osebx.dropna(inplace=True)  # Drop the first row where return cannot be calculated\n",
    "\n",
    "# Rename columns to \"Date\"\n",
    "zero_coupon.rename(columns={\"TIME_PERIOD\": \"Date\"}, inplace=True)\n",
    "exchange_rates.rename(columns={\"TIME_PERIOD\": \"Date\"}, inplace=True)\n",
    "\n",
    "# Merge all datasets on 'Date'\n",
    "df = inflation_daily.merge(zero_coupon, on=\"Date\", how=\"inner\")\n",
    "df = df.merge(exchange_rates, on=\"Date\", how=\"inner\")\n",
    "df = df.merge(osebx, on=\"Date\", how=\"inner\") \n",
    "\n",
    "df = df.drop(columns=[\"Close\", \"High\", \"Low\", \"Open\", \"Volume\",\n",
    "       \"FREQ_x\", \"Frequency_x\", \"TENOR_x\", \"Tenor_x\", \"DECIMALS_x\",\n",
    "       \"FREQ_y\", \"Frequency_y\", \"BASE_CUR\", \"Base Currency\",\n",
    "       \"QUOTE_CUR\", \"Quote Currency\", \"TENOR_y\", \"Tenor_y\", \"DECIMALS_y\",\n",
    "       \"CALCULATED\", \"UNIT_MULT\", \"Unit Multiplier\", \"COLLECTION\",\n",
    "       \"Collection Indicator\"])\n",
    "\n",
    "df.rename(columns={\"OBS_VALUE_x\": \"zero_coupon_rate\"}, inplace=True)\n",
    "df.rename(columns={\"OBS_VALUE_y\": \"usd_nok_exchange_rate\"}, inplace=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multicollinearity\n",
    "Check for multicollinearity by computing the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "selected_columns = [\"kpi\", \"usd_nok_exchange_rate\", \"zero_coupon_rate\"]\n",
    "corr_matrix = df[selected_columns].corr()\n",
    "\n",
    "# Visualize the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define highly correlated values to have a correlation coefficient >0.9. Because all variables have a correlation coefficient below 0.9, we decide to keep all of the explanatory variables. However, we notice that especially between usd_nok_exchange_rate and kpi and between zero_coupon_rate and kpi the correlation coefficient is close to the boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stationarity\n",
    "After having checked for correlation, we check for stationarity by applying the Augmented Dickey-Fuller (ADF) test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "# Check for stationarity \n",
    "def adf_test(series, var_name):\n",
    "    result = adfuller(series.dropna())  \n",
    "    status = \"Stationary\" if result[1] < 0.05 else \"Non-stationary\"\n",
    "\n",
    "    return [var_name, f\"{result[0]:.4f}\", f\"{result[1]:.4f}\", status]\n",
    "\n",
    "# Run ADF tests on all relevant variables\n",
    "stationarity = [\n",
    "    adf_test(df[\"log_return\"], \"Log Return\"),\n",
    "    adf_test(df[\"zero_coupon_rate\"], \"Zero Coupon Rate\"),\n",
    "    adf_test(df[\"usd_nok_exchange_rate\"], \"USD/NOK Exchange Rate\"),\n",
    "    adf_test(df[\"kpi\"], \"CPI\")\n",
    "]\n",
    "\n",
    "# Print results in a formatted table\n",
    "headers = [\"Variable\", \"ADF Statistic\", \"p-value\", \"Result\"]\n",
    "print(tabulate(stationarity, headers=headers, tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that the variables \"zero coupon rate,\" \"USD/NOK exchange rate,\" and \"CPI\" are non-stationary. To address this, we apply first differences to these columns, ensuring stationarity in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to stationary variables by applying first differences\n",
    "df[\"d_kpi\"] = df[\"kpi\"].diff()\n",
    "df[\"d_zero_coupon_rate\"] = df[\"zero_coupon_rate\"].diff()\n",
    "df[\"d_usd_nok_exchange_rate\"] = df[\"usd_nok_exchange_rate\"].diff()\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Run ADF tests after applying first differences on all relevant variables\n",
    "post_diff_stationarity = [\n",
    "    adf_test(df[\"d_zero_coupon_rate\"], \"Zero Coupon Rate\"),\n",
    "    adf_test(df[\"d_usd_nok_exchange_rate\"], \"USD/NOK Exchange Rate\"),\n",
    "    adf_test(df[\"d_kpi\"], \"CPI\")\n",
    "]\n",
    "\n",
    "# Print results in a table format\n",
    "headers = [\"Variable\", \"ADF Statistic\", \"p-value\", \"Stationarity\"]\n",
    "print(tabulate(post_diff_stationarity, headers=headers, tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that all the variables now are stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split percentage (e.g., 80% train, 20% test)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "\n",
    "# Split the data\n",
    "train_df = df.iloc[:split_idx].copy()  # First 80% for training\n",
    "test_df = df.iloc[split_idx:].copy()   # Last 20% for testing\n",
    "\n",
    "# Define training and testing sets\n",
    "X_train, y_train = train_df[['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']], train_df['log_return']\n",
    "X_test, y_test = test_df[['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']], test_df['log_return']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression (MLR)\n",
    "### Autocorrelation\n",
    "Start by fitting a model to the training set, and then proceed to check for autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant for intercept\n",
    "X_train_ols = sm.add_constant(X_train)\n",
    "\n",
    "# Train OLS model\n",
    "ols_model = sm.OLS(y_train, X_train_ols).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to test for autocorrelation using Breusch-Godfrey test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breusch_godfrey_test(model, nlags=1):\n",
    "    # Perform the Breusch-Godfrey test\n",
    "    lm_stat, p_value, f_stat, f_p_value = acorr_breusch_godfrey(model, nlags=nlags)\n",
    "\n",
    "    # Print the test name and results\n",
    "    print(\"Breusch-Godfrey Test for Autocorrelation\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"LM Statistic: {lm_stat:.4f}\")\n",
    "    print(f\"P-Value: {p_value:.4f}\")\n",
    "    print(f\"F-Statistic: {f_stat:.4f}\")\n",
    "    print(f\"F-Test P-Value: {f_p_value:.4f}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Interpretation of the p-value\n",
    "    if p_value < 0.05:\n",
    "        print(\"Autocorrelation detected in residuals (reject H0).\")\n",
    "    else:\n",
    "        print(\"No significant autocorrelation detected (fail to reject H0).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if autocorrelation is present in our model         \n",
    "breusch_godfrey_test(ols_model, nlags=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value of the Lagrange Multiplier statistic (LM) is \\<0.05 we reject H0. Since the p-value of the F-statistic is also \\< 0.05, it suggest that adding lagged variables could improve the model by removing autocorrelation. \n",
    "\n",
    "To get an impression of how many lags to use we visualize the partial autocorrelation function (PACF). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PACF for 'log_return'\n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_pacf(df['log_return'], lags=21, method='ywm')\n",
    "plt.title(\"Partial Autocorrelation Function (PACF)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the partial autocorrelation coefficient for each lag, with the blue dashed lines representing the 95% confidence interval. Any bar extending this interval indicates statistical significance. From the figure, we observe that none of the lags appear to be highly significant. Therefore, we will continue adding lags until we no longer detect autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added lags\n",
    "lags = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
    "variables = ['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']\n",
    "\n",
    "# Create lagged variables using a loop\n",
    "for var in variables:\n",
    "    for lag in lags:\n",
    "        train_df[f'{var}_lag{lag}'] = train_df[var].shift(lag)\n",
    "\n",
    "# Drop NaN values introduced by lagging\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# Define independent variables dynamically\n",
    "X = train_df[[col for col in train_df.columns if col.startswith('d_kpi') or \n",
    "               col.startswith('d_zero_coupon_rate') or \n",
    "               col.startswith('d_usd_nok_exchange_rate')]]\n",
    "\n",
    "\n",
    "y_train = train_df['log_return']\n",
    "\n",
    "# Add a constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the new OLS model\n",
    "lagged_model = sm.OLS(y_train, X).fit()\n",
    "\n",
    "# Check for autocorrelation again\n",
    "breusch_godfrey_test(lagged_model, nlags=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to add 28 lags to remove autocorrelation. However, including lagged variables to address autocorrelation introduces some issues. It violates the assumption that the explanatory variables are non-stochastic, and while it address autocorrelation, it also makes the model harder to interpret (SOURCE? Brooks, page 284-285) \n",
    "\n",
    "Given these problems, we revisited the PACF and observed that lag 7 and lag 18 appeared to be the most significant. Therefore, we decided to proceed with a model that includes only lag 7 and lag 18, even though autocorrelation is present. While ignoring autocorrelation keeps the coefficient estimates unbiased, the standard error estimates could be wrong. This could lead to unrealiable significance tests and potentially misleading conclusions (SOURCE?, Brooks, p. 276). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lags we want to keep\n",
    "keep_lags = [7, 18]\n",
    "\n",
    "# Keep only the original variables and the selected lagged variables\n",
    "columns_to_keep = ['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']  # Original columns\n",
    "columns_to_keep += [f'{var}_lag{lag}' for var in variables for lag in keep_lags]  # Only lags 7 & 18\n",
    "\n",
    "# Select only the necessary columns\n",
    "train_df = train_df[columns_to_keep + ['log_return']]\n",
    "\n",
    "# Drop NaN values introduced by lagging\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# Define independent and dependent variables\n",
    "X = train_df.drop(columns=['log_return'])\n",
    "y_train = train_df['log_return']\n",
    "\n",
    "# Add a constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the new OLS model\n",
    "keep_lags_model = sm.OLS(y_train, X).fit()\n",
    "\n",
    "breusch_godfrey_test(keep_lags_model, nlags=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heteroscedasticity\n",
    "We test for heteroscedasticity using the ARCH test, as it is well-suited for time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract residuals\n",
    "residuals = keep_lags_model.resid\n",
    "\n",
    "# Perform ARCH test\n",
    "arch_test = het_arch(residuals)\n",
    "\n",
    "# Print results\n",
    "print(f\"ARCH Test Statistic: {arch_test[0]}\")\n",
    "print(f\"p-value: {arch_test[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with heteroscedasticity we use heteroscedasticity-consistent standard error estimates.\n",
    "HC3 is conservative and recommended for larger sample sizes. Since our sample size is approximately 2000 observations it can be considered medium to large, and HC3 could be a good choice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute robust standard errors (Huber-White/HC3 for larger samples)\n",
    "model_robust = keep_lags_model.get_robustcov_results(cov_type='HC3')\n",
    "\n",
    "# Print summary with robust standard errors\n",
    "print(model_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.copy()\n",
    "\n",
    "# Ensure that lags 7 and 18 exist in the test set\n",
    "X_test['d_kpi_lag7'] = X_test['d_kpi'].shift(7)\n",
    "X_test['d_zero_coupon_rate_lag7'] = X_test['d_zero_coupon_rate'].shift(7)\n",
    "X_test['d_usd_nok_exchange_rate_lag7'] = X_test['d_usd_nok_exchange_rate'].shift(7)\n",
    "\n",
    "X_test['d_kpi_lag18'] = X_test['d_kpi'].shift(18)\n",
    "X_test['d_zero_coupon_rate_lag18'] = X_test['d_zero_coupon_rate'].shift(18)\n",
    "X_test['d_usd_nok_exchange_rate_lag18'] = X_test['d_usd_nok_exchange_rate'].shift(18)\n",
    "\n",
    "# Drop rows with NaN values (first 18 rows will have NaNs due to lags)\n",
    "X_test = X_test.dropna()\n",
    "y_test = y_test.loc[X_test.index]  # Ensure y_test aligns with new X_test\n",
    "\n",
    "# Add constant for intercept\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_robust.predict(X_test)\n",
    "\n",
    "# Compute performance metrics\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Mean Absolute Percentage Error\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
    "\n",
    "# Print results\n",
    "print(f\"MAPE: {mape:.4f}%\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Elias og Erik"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
