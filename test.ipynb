{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIØ4317 Empirical Mini Project\n",
    "## Introduction\n",
    "Hva er dette prosjektet, formål osv. forklare litt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_breusch_godfrey, het_arch\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading the different csv-files that contain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "data_path = \"data/\"\n",
    "\n",
    "# Load datasets\n",
    "zero_coupon = pd.read_csv(os.path.join(data_path, \"zero_coupon_rates.csv\"), delimiter=\";\")\n",
    "exchange_rates = pd.read_csv(os.path.join(data_path, \"usd_nok.csv\"), delimiter=\";\")\n",
    "inflation = pd.read_csv(os.path.join(data_path, \"kpi.csv\"), delimiter=\";\")\n",
    "osebx = pd.read_csv(os.path.join(data_path, \"osebx_prices.csv\"), delimiter=\";\")\n",
    "\n",
    "# Replace commas with dots in the KPI column and convert to float\n",
    "inflation[\"kpi\"] = inflation[\"kpi\"].str.replace(\",\", \".\").astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we check for missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"OSEBX\": osebx,\n",
    "    \"Zero Coupon\": zero_coupon,\n",
    "    \"Exchange Rates\": exchange_rates,\n",
    "    \"Inflation\": inflation\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nMissing Values in {name} Dataset:\")\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the data does not contain any null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with converting the dates to Datetime format. Also, for inflation, since we are originally only having monthly data, we use forward-fill to get daily inflation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to Datetime format\n",
    "osebx[\"Date\"] = pd.to_datetime(osebx[\"Date\"])\n",
    "zero_coupon[\"TIME_PERIOD\"] = pd.to_datetime(zero_coupon[\"TIME_PERIOD\"])\n",
    "exchange_rates[\"TIME_PERIOD\"] = pd.to_datetime(exchange_rates[\"TIME_PERIOD\"])\n",
    "inflation[\"Date\"] = pd.to_datetime(inflation[\"Date\"], format=\"%YM%m\")\n",
    "\n",
    "# Create a full date range from the first to last available date in your dataset\n",
    "full_date_range = pd.date_range(start=inflation[\"Date\"].min(), end=inflation[\"Date\"].max(), freq=\"D\")\n",
    "\n",
    "# Create a DataFrame with daily dates\n",
    "inflation_daily = pd.DataFrame({\"Date\": full_date_range})\n",
    "\n",
    "# Merge with the original inflation data (left join) and forward-fill missing values\n",
    "inflation_daily = inflation_daily.merge(inflation, on=\"Date\", how=\"left\")\n",
    "inflation_daily[\"kpi\"] = inflation_daily[\"kpi\"].interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute log returns for OSEBX. Also, rename columns to ensure that all dataframes have a column named \"Date\", so that we can merge all datasets on \"Date\". After having merged all the datasets to one dataframe, we drop all the columns we are not interested in. Consequently, the columns we are left with are \"Date\", \"kpi\", \"zero_coupon_rate\", \"usd_nok_exchange_rate\", and \"log_return\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log returns\n",
    "osebx[\"log_return\"] = np.log(osebx[\"Close\"] / osebx[\"Close\"].shift(1))\n",
    "osebx.dropna(inplace=True)  # Drop the first row where return cannot be calculated\n",
    "\n",
    "# Rename columns to \"Date\"\n",
    "zero_coupon.rename(columns={\"TIME_PERIOD\": \"Date\"}, inplace=True)\n",
    "exchange_rates.rename(columns={\"TIME_PERIOD\": \"Date\"}, inplace=True)\n",
    "\n",
    "# Merge all datasets on 'Date'\n",
    "df = inflation_daily.merge(zero_coupon, on=\"Date\", how=\"inner\")\n",
    "df = df.merge(exchange_rates, on=\"Date\", how=\"inner\")\n",
    "df = df.merge(osebx, on=\"Date\", how=\"inner\") \n",
    "\n",
    "df = df.drop(columns=[\"Close\", \"High\", \"Low\", \"Open\", \"Volume\",\n",
    "       \"FREQ_x\", \"Frequency_x\", \"TENOR_x\", \"Tenor_x\", \"DECIMALS_x\",\n",
    "       \"FREQ_y\", \"Frequency_y\", \"BASE_CUR\", \"Base Currency\",\n",
    "       \"QUOTE_CUR\", \"Quote Currency\", \"TENOR_y\", \"Tenor_y\", \"DECIMALS_y\",\n",
    "       \"CALCULATED\", \"UNIT_MULT\", \"Unit Multiplier\", \"COLLECTION\",\n",
    "       \"Collection Indicator\"])\n",
    "\n",
    "df.rename(columns={\"OBS_VALUE_x\": \"zero_coupon_rate\"}, inplace=True)\n",
    "df.rename(columns={\"OBS_VALUE_y\": \"usd_nok_exchange_rate\"}, inplace=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for multicollinearity by computing the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "selected_columns = [\"kpi\", \"usd_nok_exchange_rate\", \"zero_coupon_rate\"]\n",
    "corr_matrix = df[selected_columns].corr()\n",
    "\n",
    "# Visualize the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define highly correlated values to have a correlation coefficient >0.9. Because all variables have a correlation coefficient below 0.9, we decide to keep all of the explanatory variables. However, we notice that especially between usd_nok_exchange_rate and kpi and between zero_coupon_rate and kpi the correlation coefficient is close to the boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having checked for correlation, we check for stationarity by applying the Augmented Dickey-Fuller (ADF) test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for stationarity \n",
    "def adf_test(series, var_name):\n",
    "    result = adfuller(series.dropna())  \n",
    "    print(\"=\"*40)  # Separator line\n",
    "    print(f\"ADF Test for {var_name}:\")\n",
    "    print(f\"ADF Statistic: {result[0]:.4f}\")\n",
    "    print(f\"p-value: {result[1]:.4f}\")\n",
    "    print(\"Stationary\" if result[1] < 0.05 else \"Non-stationary\")\n",
    "    print(\"=\"*40 + \"\\n\")  # End separator\n",
    "\n",
    "# Run ADF tests on all relevant variables\n",
    "adf_test(df[\"log_return\"], \"Log Return\")\n",
    "adf_test(df[\"zero_coupon_rate\"], \"Zero Coupon Rate\")  \n",
    "adf_test(df[\"usd_nok_exchange_rate\"], \"USD/NOK Exchange Rate\")  \n",
    "adf_test(df[\"kpi\"], \"CPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the variables \"zero coupon rate\", \"USD/NOK exchange rate\", and \"CPI\" are non-stationary. To get these variables to be stationary, we apply first differences to these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to stationary variables by applying first differences\n",
    "df[\"d_kpi\"] = df[\"kpi\"].diff()\n",
    "df[\"d_zero_coupon_rate\"] = df[\"zero_coupon_rate\"].diff()\n",
    "df[\"d_usd_nok_exchange_rate\"] = df[\"usd_nok_exchange_rate\"].diff()\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "adf_test(df[\"d_zero_coupon_rate\"], \"Differenced Zero Coupon Rate\") \n",
    "adf_test(df[\"d_usd_nok_exchange_rate\"], \"Differenced USD/NOK Exchange Rate\") \n",
    "adf_test(df[\"d_kpi\"], \"Differenced CPI\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that all the variables now are stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split percentage (e.g., 80% train, 20% test)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "\n",
    "# Split the data\n",
    "train_df = df.iloc[:split_idx].copy()  # First 80% for training\n",
    "test_df = df.iloc[split_idx:].copy()   # Last 20% for testing\n",
    "\n",
    "# Define training and testing sets\n",
    "X_train, y_train = train_df[['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']], train_df['log_return']\n",
    "X_test, y_test = test_df[['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']], test_df['log_return']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression (MLR)\n",
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant for intercept\n",
    "X_train_ols = sm.add_constant(X_train)\n",
    "\n",
    "# Train OLS model\n",
    "ols_model = sm.OLS(y_train, X_train_ols).fit()\n",
    "\n",
    "# Print summary\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for autocorrelation using Breusch-Godfrey test\n",
    "\n",
    "NOTE: Se på hva LM, F osv. er. Nødvendig å ha med?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breusch_godfrey_test(model, nlags=1):\n",
    "   \n",
    "    # Perform the test\n",
    "    lm_stat, p_value, f_stat, f_p_value = acorr_breusch_godfrey(model, nlags=nlags)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Breusch-Godfrey Test (nlags={nlags})\")\n",
    "    print(f\"LM Statistic: {lm_stat:.4f}\")\n",
    "    print(f\"P-Value: {p_value:.4f}\")\n",
    "    print(f\"F-Statistic: {f_stat:.4f}\")\n",
    "    print(f\"F-Test P-Value: {f_p_value:.4f}\")\n",
    "\n",
    "    # Interpretation\n",
    "    if p_value < 0.05:\n",
    "        print(\"Autocorrelation detected in residuals (reject H0).\")\n",
    "    else:\n",
    "        print(\"No significant autocorrelation detected (fail to reject H0).\")\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        \"LM Statistic\": lm_stat,\n",
    "        \"P-Value\": p_value,\n",
    "        \"F-Statistic\": f_stat,\n",
    "        \"F-Test P-Value\": f_p_value\n",
    "    }\n",
    "\n",
    "breusch_godfrey_test(ols_model, nlags=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with autocorrelation we will try to add lagged variables. To get an impression of how many lags to use we visualize the partial autocorrelation function (PACF). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PACF for 'log_return'\n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_pacf(df['log_return'], lags=21, method='ywm')\n",
    "plt.title(\"Partial Autocorrelation Function (PACF)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the partial autocorrelation coefficient for each lag, with the blue dashed lines representing the 95% confidence interval. Any bar extending this interval indicates statistical significance. From the figure, we observe that lag 7 and lag 18 seems to be most significant. \n",
    "\n",
    "We try to add lagged variables for lag 7 and lag 18 to remove autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add lags until no autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lags you want to create\n",
    "lags = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "variables = ['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']\n",
    "\n",
    "# Create lagged variables using a loop\n",
    "for var in variables:\n",
    "    for lag in lags:\n",
    "        train_df[f'{var}_lag{lag}'] = train_df[var].shift(lag)\n",
    "\n",
    "# Drop NaN values introduced by lagging\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# Define independent variables dynamically\n",
    "X = train_df[[col for col in train_df.columns if col.startswith('d_kpi') or \n",
    "               col.startswith('d_zero_coupon_rate') or \n",
    "               col.startswith('d_usd_nok_exchange_rate')]]\n",
    "\n",
    "\n",
    "y_train = train_df['log_return']\n",
    "\n",
    "# Add a constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the new OLS model\n",
    "lagged_model = sm.OLS(y_train, X).fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for autocorrelation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breusch_godfrey_test(lagged_model, nlags=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with using model with only lags 7 and 18. Bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lags we want to keep\n",
    "keep_lags = [7, 18]\n",
    "\n",
    "# Keep only the original variables and the selected lagged variables\n",
    "columns_to_keep = ['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']  # Original columns\n",
    "columns_to_keep += [f'{var}_lag{lag}' for var in variables for lag in keep_lags]  # Only lags 7 & 18\n",
    "\n",
    "# Select only the necessary columns\n",
    "train_df = train_df[columns_to_keep + ['log_return']]\n",
    "\n",
    "# Drop NaN values introduced by lagging\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# Define independent and dependent variables\n",
    "X = train_df.drop(columns=['log_return'])\n",
    "y_train = train_df['log_return']\n",
    "\n",
    "# Add a constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the new OLS model\n",
    "keep_lags_model = sm.OLS(y_train, X).fit()\n",
    "\n",
    "breusch_godfrey_test(keep_lags_model, nlags=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heteroscedasticity\n",
    "We test for heteroscedasticity using the ARCH test, as it is well-suited for time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract residuals\n",
    "residuals = keep_lags_model.resid\n",
    "\n",
    "# Perform ARCH test\n",
    "arch_test = het_arch(residuals)\n",
    "\n",
    "# Print results\n",
    "print(f\"ARCH Test Statistic: {arch_test[0]}\")\n",
    "print(f\"p-value: {arch_test[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with heteroscedasticity we use heteroscedasticity-consistent standard error estimates.\n",
    "HC3 is conservative and recommended for larger sample sizes. Since our sample size is approximately 2000 observations it can be considered medium to large, and HC3 could be a good choice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute robust standard errors (Huber-White/HC3 for larger samples)\n",
    "model_robust = keep_lags_model.get_robustcov_results(cov_type='HC3')\n",
    "\n",
    "# Print summary with robust standard errors\n",
    "print(model_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.copy()\n",
    "\n",
    "# Ensure that lags 7 and 18 exist in the test set\n",
    "X_test['d_kpi_lag7'] = X_test['d_kpi'].shift(7)\n",
    "X_test['d_zero_coupon_rate_lag7'] = X_test['d_zero_coupon_rate'].shift(7)\n",
    "X_test['d_usd_nok_exchange_rate_lag7'] = X_test['d_usd_nok_exchange_rate'].shift(7)\n",
    "\n",
    "X_test['d_kpi_lag18'] = X_test['d_kpi'].shift(18)\n",
    "X_test['d_zero_coupon_rate_lag18'] = X_test['d_zero_coupon_rate'].shift(18)\n",
    "X_test['d_usd_nok_exchange_rate_lag18'] = X_test['d_usd_nok_exchange_rate'].shift(18)\n",
    "\n",
    "# Drop rows with NaN values (first 18 rows will have NaNs due to lags)\n",
    "X_test = X_test.dropna()\n",
    "y_test = y_test.loc[X_test.index]  # Ensure y_test aligns with new X_test\n",
    "\n",
    "# Add constant for intercept\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_robust.predict(X_test)\n",
    "\n",
    "# Compute performance metrics\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Mean Absolute Percentage Error\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
    "\n",
    "# Print results\n",
    "print(f\"MAPE: {mape:.4f}%\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Elias og Erik\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Ensure the DataFrame is sorted by date\n",
    "df = df.sort_values(by=\"Date\").reset_index(drop=True)\n",
    "\n",
    "# Define the dependent variable and exogenous regressors\n",
    "y = df[\"log_return\"]\n",
    "X = df[[\"zero_coupon_rate\", \"usd_nok_exchange_rate\", \"kpi\"]]\n",
    "\n",
    "# Split the data into training (80%) and testing (final 20%) sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "\n",
    "# Define ARIMAX order parameters (p, d, q)\n",
    "# This is a starting guess; you may refine these using AIC/BIC or a grid search\n",
    "order = (1, 0, 1)\n",
    "\n",
    "# Fit the ARIMAX model\n",
    "arimax_model = SARIMAX(endog=y_train, exog=X_train, order=order,\n",
    "                       enforce_stationarity=False,\n",
    "                       enforce_invertibility=False).fit(disp=False)\n",
    "\n",
    "# Print the ARIMAX model summary\n",
    "print(arimax_model.summary())\n",
    "\n",
    "# Ensure ARIMAX has been trained & has predictions\n",
    "forecast = arimax_model.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1, exog=X_test)\n",
    "\n",
    "# Compute R² for ARIMAX\n",
    "r2_arimax = r2_score(y_test, forecast)\n",
    "print(f\"ARIMAX R²: {r2_arimax:.4f}\")\n",
    "\n",
    "\n",
    "# Forecasting on the test set\n",
    "# Ensure that X_test is aligned with the forecast period\n",
    "forecast = arimax_model.predict(start=len(y_train),\n",
    "                                end=len(y_train) + len(y_test) - 1,\n",
    "                                exog=X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to check for multicolinearity using the the variance inflation factor (VIF). R^2 value is negative in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Compute VIF for each predictor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_train.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to difference the data since VIF >> 10 for both KPI and exchange rate, and zero coupon rate is also > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute first differences to remove trends\n",
    "df[\"diff_kpi\"] = df[\"kpi\"].diff()\n",
    "df[\"diff_usd_nok\"] = df[\"usd_nok_exchange_rate\"].diff()\n",
    "df[\"diff_zero_coupon\"] = df[\"zero_coupon_rate\"].diff()\n",
    "\n",
    "# Drop NaN values created by differencing\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Define dependent variable (unchanged)\n",
    "y_df = df[\"log_return\"]\n",
    "\n",
    "# Use differenced exogenous variables\n",
    "X_df = df[[\"diff_zero_coupon\", \"diff_usd_nok\", \"diff_kpi\"]]\n",
    "\n",
    "# Split into training and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "y_train_df, y_test_df = y_df[:train_size], y_df[train_size:]\n",
    "X_train_df, X_test_df = X_df[:train_size], X_df[train_size:]\n",
    "\n",
    "# Compute VIF for each predictor\n",
    "vif_data_df = pd.DataFrame()\n",
    "vif_data_df[\"Feature\"] = X_train.columns\n",
    "vif_data_df[\"VIF\"] = [variance_inflation_factor(X_train_df.values, i) for i in range(X_train.shape[1])]\n",
    "\n",
    "print(vif_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the differenced data, lets try setting up the ARIMAX model again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ARIMAX order\n",
    "order = (1, 0, 1) \n",
    "\n",
    "# Fit the ARIMAX model\n",
    "arimax_model = SARIMAX(endog=y_train, exog=X_train, order=order,\n",
    "                       enforce_stationarity=False,\n",
    "                       enforce_invertibility=False).fit(disp=False)\n",
    "\n",
    "# Print model summary\n",
    "print(arimax_model.summary())\n",
    "\n",
    "# Forecasting\n",
    "forecast = arimax_model.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1, exog=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to iterate through different orders to find the optimal one. (Denne er nok litt for eksperimentell i beste case, her tror jeg kunnskapen om PACF og ACF er mer riktig XD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import itertools\n",
    "\n",
    "# Define the range of (p, d, q) values to test\n",
    "p_values = range(0, 4)  # Test AR terms from 0 to 3\n",
    "d_values = [0]          # Assume stationarity (no differencing)\n",
    "q_values = range(0, 4)  # Test MA terms from 0 to 3\n",
    "\n",
    "best_r2 = -float(\"inf\")  # Start with a very low R²\n",
    "best_order = None\n",
    "results = []\n",
    "\n",
    "# Loop over all (p, d, q) combinations\n",
    "for p, d, q in itertools.product(p_values, d_values, q_values):\n",
    "    try:\n",
    "        # Fit the ARIMAX model with given (p, d, q)\n",
    "        model = SARIMAX(y_train_df, exog=X_train_df, order=(p, d, q),\n",
    "                        enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "\n",
    "        # Generate predictions\n",
    "        forecast = model.predict(start=len(y_train_df), end=len(y_train_df) + len(y_test_df) - 1, exog=X_test_df)\n",
    "\n",
    "        # Compute R²\n",
    "        r2 = r2_score(y_test_df, forecast)\n",
    "\n",
    "        # Store results\n",
    "        results.append((p, d, q, r2))\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_order = (p, d, q)\n",
    "\n",
    "        print(f\"Tested ARIMAX({p},{d},{q}) → R²: {r2:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping ARIMAX({p},{d},{q}) due to error: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results, columns=[\"p\", \"d\", \"q\", \"R2\"]).sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the resulting forecast against the test data. Using the differenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_train_df.index, y_train_df, label=\"Training Data\")\n",
    "plt.plot(y_test_df.index, y_test_df, label=\"Test Data\", color='gray')\n",
    "plt.plot(y_test_df.index, forecast_df, label=\"ARIMAX Forecast\", color='red')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Log Return\")\n",
    "plt.title(\"ARIMAX Model Forecast vs Actual\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
