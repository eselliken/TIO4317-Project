{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIØ4317 Empirical Mini Project\n",
    "## Introduction\n",
    "Hva er dette prosjektet, formål osv. forklare litt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_breusch_godfrey, het_arch\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading the different csv-files that contain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "data_path = \"data/\"\n",
    "\n",
    "# Load datasets\n",
    "zero_coupon = pd.read_csv(os.path.join(data_path, \"zero_coupon_rates.csv\"), delimiter=\";\")\n",
    "exchange_rates = pd.read_csv(os.path.join(data_path, \"usd_nok.csv\"), delimiter=\";\")\n",
    "inflation = pd.read_csv(os.path.join(data_path, \"kpi.csv\"), delimiter=\";\")\n",
    "osebx = pd.read_csv(os.path.join(data_path, \"osebx_prices.csv\"), delimiter=\";\")\n",
    "\n",
    "# Replace commas with dots in the KPI column and convert to float\n",
    "inflation[\"kpi\"] = inflation[\"kpi\"].str.replace(\",\", \".\").astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we check for missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"OSEBX\": osebx,\n",
    "    \"Zero Coupon\": zero_coupon,\n",
    "    \"Exchange Rates\": exchange_rates,\n",
    "    \"Inflation\": inflation\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nMissing Values in {name} Dataset:\")\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the data does not contain any null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with converting the dates to Datetime format. Also, for inflation, since we are originally only having monthly data, we use forward-fill to get daily inflation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to Datetime format\n",
    "osebx[\"Date\"] = pd.to_datetime(osebx[\"Date\"])\n",
    "zero_coupon[\"TIME_PERIOD\"] = pd.to_datetime(zero_coupon[\"TIME_PERIOD\"])\n",
    "exchange_rates[\"TIME_PERIOD\"] = pd.to_datetime(exchange_rates[\"TIME_PERIOD\"])\n",
    "inflation[\"Date\"] = pd.to_datetime(inflation[\"Date\"], format=\"%YM%m\")\n",
    "\n",
    "# Create a full date range from the first to last available date in your dataset\n",
    "full_date_range = pd.date_range(start=inflation[\"Date\"].min(), end=inflation[\"Date\"].max(), freq=\"D\")\n",
    "\n",
    "# Create a DataFrame with daily dates\n",
    "inflation_daily = pd.DataFrame({\"Date\": full_date_range})\n",
    "\n",
    "# Merge with the original inflation data (left join) and forward-fill missing values\n",
    "inflation_daily = inflation_daily.merge(inflation, on=\"Date\", how=\"left\")\n",
    "inflation_daily[\"kpi\"] = inflation_daily[\"kpi\"].interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute log returns for OSEBX. Also, rename columns to ensure that all dataframes have a column named \"Date\", so that we can merge all datasets on \"Date\". After having merged all the datasets to one dataframe, we drop all the columns we are not interested in. Consequently, the columns we are left with are \"Date\", \"kpi\", \"zero_coupon_rate\", \"usd_nok_exchange_rate\", and \"log_return\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log returns\n",
    "osebx[\"log_return\"] = np.log(osebx[\"Close\"] / osebx[\"Close\"].shift(1))\n",
    "osebx.dropna(inplace=True)  # Drop the first row where return cannot be calculated\n",
    "\n",
    "# Rename columns to \"Date\"\n",
    "zero_coupon.rename(columns={\"TIME_PERIOD\": \"Date\"}, inplace=True)\n",
    "exchange_rates.rename(columns={\"TIME_PERIOD\": \"Date\"}, inplace=True)\n",
    "\n",
    "# Merge all datasets on 'Date'\n",
    "df = inflation_daily.merge(zero_coupon, on=\"Date\", how=\"inner\")\n",
    "df = df.merge(exchange_rates, on=\"Date\", how=\"inner\")\n",
    "df = df.merge(osebx, on=\"Date\", how=\"inner\") \n",
    "\n",
    "df = df.drop(columns=[\"Close\", \"High\", \"Low\", \"Open\", \"Volume\",\n",
    "       \"FREQ_x\", \"Frequency_x\", \"TENOR_x\", \"Tenor_x\", \"DECIMALS_x\",\n",
    "       \"FREQ_y\", \"Frequency_y\", \"BASE_CUR\", \"Base Currency\",\n",
    "       \"QUOTE_CUR\", \"Quote Currency\", \"TENOR_y\", \"Tenor_y\", \"DECIMALS_y\",\n",
    "       \"CALCULATED\", \"UNIT_MULT\", \"Unit Multiplier\", \"COLLECTION\",\n",
    "       \"Collection Indicator\"])\n",
    "\n",
    "df.rename(columns={\"OBS_VALUE_x\": \"zero_coupon_rate\"}, inplace=True)\n",
    "df.rename(columns={\"OBS_VALUE_y\": \"usd_nok_exchange_rate\"}, inplace=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for multicollinearity by computing the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "selected_columns = [\"kpi\", \"usd_nok_exchange_rate\", \"zero_coupon_rate\"]\n",
    "corr_matrix = df[selected_columns].corr()\n",
    "\n",
    "# Visualize the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define highly correlated values to have a correlation coefficient >0.9. Because all variables have a correlation coefficient below 0.9, we decide to keep all of the explanatory variables. However, we notice that especially between usd_nok_exchange_rate and kpi and between zero_coupon_rate and kpi the correlation coefficient is close to the boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having checked for correlation, we check for stationarity by applying the Augmented Dickey-Fuller (ADF) test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for stationarity \n",
    "def adf_test(series, var_name):\n",
    "    result = adfuller(series.dropna())  \n",
    "    print(\"=\"*40)  # Separator line\n",
    "    print(f\"ADF Test for {var_name}:\")\n",
    "    print(f\"ADF Statistic: {result[0]:.4f}\")\n",
    "    print(f\"p-value: {result[1]:.4f}\")\n",
    "    print(\"Stationary\" if result[1] < 0.05 else \"Non-stationary\")\n",
    "    print(\"=\"*40 + \"\\n\")  # End separator\n",
    "\n",
    "# Run ADF tests on all relevant variables\n",
    "adf_test(df[\"log_return\"], \"Log Return\")\n",
    "adf_test(df[\"zero_coupon_rate\"], \"Zero Coupon Rate\")  \n",
    "adf_test(df[\"usd_nok_exchange_rate\"], \"USD/NOK Exchange Rate\")  \n",
    "adf_test(df[\"kpi\"], \"CPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the variables \"zero coupon rate\", \"USD/NOK exchange rate\", and \"CPI\" are non-stationary. To get these variables to be stationary, we apply first differences to these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to stationary variables by applying first differences\n",
    "df[\"d_kpi\"] = df[\"kpi\"].diff()\n",
    "df[\"d_zero_coupon_rate\"] = df[\"zero_coupon_rate\"].diff()\n",
    "df[\"d_usd_nok_exchange_rate\"] = df[\"usd_nok_exchange_rate\"].diff()\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "adf_test(df[\"d_zero_coupon_rate\"], \"Differenced Zero Coupon Rate\") \n",
    "adf_test(df[\"d_usd_nok_exchange_rate\"], \"Differenced USD/NOK Exchange Rate\") \n",
    "adf_test(df[\"d_kpi\"], \"Differenced CPI\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that all the variables now are stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split percentage (e.g., 80% train, 20% test)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "\n",
    "# Split the data\n",
    "train_df = df.iloc[:split_idx].copy()  # First 80% for training\n",
    "test_df = df.iloc[split_idx:].copy()   # Last 20% for testing\n",
    "\n",
    "# Define training and testing sets\n",
    "X_train, y_train = train_df[['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']], train_df['log_return']\n",
    "X_test, y_test = test_df[['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']], test_df['log_return']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression (MLR)\n",
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant for intercept\n",
    "X_train_ols = sm.add_constant(X_train)\n",
    "\n",
    "# Train OLS model\n",
    "ols_model = sm.OLS(y_train, X_train_ols).fit()\n",
    "\n",
    "# Print summary\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for autocorrelation using Breusch-Godfrey test\n",
    "\n",
    "NOTE: Se på hva LM, F osv. er. Nødvendig å ha med?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breusch_godfrey_test(model, nlags=1):\n",
    "   \n",
    "    # Perform the test\n",
    "    lm_stat, p_value, f_stat, f_p_value = acorr_breusch_godfrey(model, nlags=nlags)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Breusch-Godfrey Test (nlags={nlags})\")\n",
    "    print(f\"LM Statistic: {lm_stat:.4f}\")\n",
    "    print(f\"P-Value: {p_value:.4f}\")\n",
    "    print(f\"F-Statistic: {f_stat:.4f}\")\n",
    "    print(f\"F-Test P-Value: {f_p_value:.4f}\")\n",
    "\n",
    "    # Interpretation\n",
    "    if p_value < 0.05:\n",
    "        print(\"Autocorrelation detected in residuals (reject H0).\")\n",
    "    else:\n",
    "        print(\"No significant autocorrelation detected (fail to reject H0).\")\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        \"LM Statistic\": lm_stat,\n",
    "        \"P-Value\": p_value,\n",
    "        \"F-Statistic\": f_stat,\n",
    "        \"F-Test P-Value\": f_p_value\n",
    "    }\n",
    "\n",
    "breusch_godfrey_test(ols_model, nlags=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with autocorrelation we will try to add lagged variables. To get an impression of how many lags to use we visualize the partial autocorrelation function (PACF). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PACF for 'log_return'\n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_pacf(df['log_return'], lags=21, method='ywm')\n",
    "plt.title(\"Partial Autocorrelation Function (PACF)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the partial autocorrelation coefficient for each lag, with the blue dashed lines representing the 95% confidence interval. Any bar extending this interval indicates statistical significance. From the figure, we observe that lag 7 and lag 18 seems to be most significant. \n",
    "\n",
    "We try to add lagged variables for lag 7 and lag 18 to remove autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add lags until no autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lags you want to create\n",
    "lags = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "variables = ['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']\n",
    "\n",
    "# Create lagged variables using a loop\n",
    "for var in variables:\n",
    "    for lag in lags:\n",
    "        train_df[f'{var}_lag{lag}'] = train_df[var].shift(lag)\n",
    "\n",
    "# Drop NaN values introduced by lagging\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# Define independent variables dynamically\n",
    "X = train_df[[col for col in train_df.columns if col.startswith('d_kpi') or \n",
    "               col.startswith('d_zero_coupon_rate') or \n",
    "               col.startswith('d_usd_nok_exchange_rate')]]\n",
    "\n",
    "\n",
    "y_train = train_df['log_return']\n",
    "\n",
    "# Add a constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the new OLS model\n",
    "lagged_model = sm.OLS(y_train, X).fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for autocorrelation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breusch_godfrey_test(lagged_model, nlags=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with using model with only lags 7 and 18. Bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lags we want to keep\n",
    "keep_lags = [7, 18]\n",
    "\n",
    "# Keep only the original variables and the selected lagged variables\n",
    "columns_to_keep = ['d_kpi', 'd_zero_coupon_rate', 'd_usd_nok_exchange_rate']  # Original columns\n",
    "columns_to_keep += [f'{var}_lag{lag}' for var in variables for lag in keep_lags]  # Only lags 7 & 18\n",
    "\n",
    "# Select only the necessary columns\n",
    "train_df = train_df[columns_to_keep + ['log_return']]\n",
    "\n",
    "# Drop NaN values introduced by lagging\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# Define independent and dependent variables\n",
    "X = train_df.drop(columns=['log_return'])\n",
    "y_train = train_df['log_return']\n",
    "\n",
    "# Add a constant for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the new OLS model\n",
    "keep_lags_model = sm.OLS(y_train, X).fit()\n",
    "\n",
    "breusch_godfrey_test(keep_lags_model, nlags=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heteroskedasticity\n",
    "We continue by testing for heteroskedasticity. This is important because we assume homoskedasticity, which means that all the errors have the same variance. Identifying heteroskedasticity is important because it affects standard errors, confidence intervals, and hypothesis testing reliability in OLS regression.\n",
    "\n",
    "We test if the homoskedasticity assumption holds by using the ARCH test, as it is well-suited for time-series data because it detects time-dependent variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = keep_lags_model.resid\n",
    "\n",
    "arch_test = het_arch(residuals)\n",
    "\n",
    "print(f\"ARCH Test Statistic: {arch_test[0]}\")\n",
    "print(f\"p-value: {arch_test[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the strong heteroskedasticity identified by the ARCH test, we use heteroskedasticity-consistent (HC) standard error estimates. This is to ensure valid inference in our regression model. Since our training dataset consists of approximately 2000 observations, we choose HC3 since it is well-suited for medium to large sample sized. This improves the robustness of our statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_robust = keep_lags_model.get_robustcov_results(cov_type='HC3')\n",
    "\n",
    "print(model_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation\n",
    "Now, we want to evaluate the performance of our Multiple Linear Regression (MLR) model. In order to do this, we first need to add lag 7 and 18 to our test set.\n",
    "\n",
    "After having done this, we use the robust model to make predictions and compute the performance metrics Mean Absolute Percentage Error (MAPE), Mean Squared Error (MSE), and Mean Absolute Error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "X_test = X_test.copy()\n",
    "\n",
    "X_test['d_kpi_lag7'] = X_test['d_kpi'].shift(7)\n",
    "X_test['d_zero_coupon_rate_lag7'] = X_test['d_zero_coupon_rate'].shift(7)\n",
    "X_test['d_usd_nok_exchange_rate_lag7'] = X_test['d_usd_nok_exchange_rate'].shift(7)\n",
    "\n",
    "X_test['d_kpi_lag18'] = X_test['d_kpi'].shift(18)\n",
    "X_test['d_zero_coupon_rate_lag18'] = X_test['d_zero_coupon_rate'].shift(18)\n",
    "X_test['d_usd_nok_exchange_rate_lag18'] = X_test['d_usd_nok_exchange_rate'].shift(18)\n",
    "\n",
    "# Drop rows with NaN values (first 18 rows will have NaNs due to lags)\n",
    "X_test = X_test.dropna()\n",
    "\n",
    "# Ensure y_test aligns with new X_test\n",
    "y_test = y_test.loc[X_test.index]  \n",
    "\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "y_pred = model_robust.predict(X_test)\n",
    "\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  \n",
    "mse = mean_squared_error(y_test, y_pred)  \n",
    "mae = mean_absolute_error(y_test, y_pred) \n",
    "\n",
    "results = [\n",
    "    [\"Metric\", \"Value\"],\n",
    "    [\"MAPE (%)\", f\"{mape:.4f}\"],\n",
    "    [\"MSE\", f\"{mse:.4f}\"],\n",
    "    [\"MAE\", f\"{mae:.4f}\"]\n",
    "]\n",
    "\n",
    "print(tabulate(results, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Percentage Error (MAPE)**\n",
    "\n",
    "A MAPE value of **244.0910%** indicates that, on average, the model's predictions deviate from the actual values by more than twice their magnitude. This indicates clearly that the model struggles to make accurate predictions, and is likely due to a combination of the model not being able to capture the complex dynamics in the explanatory variables and the fact that log returns are very small values. Therefore, a small mistake in the prediction can lead to a large percentage error.\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "The MSE value of **0.0001** seems quite low, however, this is quite misleading. As noted earlier, the scale of the dependent variable is small, which makes the squared errors appear low even if the relative percentage error is large.\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "The MAE value of **0.0061** represents the average absolute error in raw terms and indicates small deviations in absolute terms. However, the reason for such a low MAE value is the small scale of the dependent variable. This makes the absolute errors to seem small even if the relative error is large. \n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "While the MSE and MAE values appear low, the extremely high MAPE highly suggests that the model performs poorly in predicting the dependent variable. The small scale of the log returns of the OSEBX makes the absolute errors seem small but leads to large percentage errors. Also, the low **adjusted R squared value of 0.113** further confirms that the model has weak explanatory power. \n",
    "\n",
    "Overall, the model is not reliable for forecasting OSEBX returns and requires improvements, such as using a more sophisticated approach like ARIMAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Elias og Erik"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
